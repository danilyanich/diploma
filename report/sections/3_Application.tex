% Применение в интеллектуальном анализе данных (ИАД) (общие слова, потом конкретно о тех задачах, на которых будут проводиться тесты)

\newpage
\chapter{Применение в интеллектуальном анализе данных}

Многие методы уменьшения размерности тесно связаны с приближениями матрицами низкого ранга,
и неотрицательная матричная факторизация является особенной в том,
что искомые матрицы низкого ранга ограничены только неотрицательными элементами.
Неотрицательность играет важное значение для данных во многих областях,
в результате чего неотрицательные матрицы низкого ранга приводят к физически естественным интерпретациям искомого приближения.
За последнее десятилетие неотрицательная матричная факторизация получила огромное внимание и была успешно применена к широкому кругу важных проблем в таких областях,
как интеллектуальный анализ текста, компьютерное зрение, биоинформатика, анализ спектральных данных, слепое разделение источников и многим другим.

Ниже будет рассмотрены некоторые алгоритмы интеллектуального анализа текста.





\newpage





\section{Интеллектуальный анализ текста}

Под анализом текста подразумеваются методы извлечения полезной информации из больших и часто неструктурированных коллекций текстов.
Другими словами - поиск информации.
Типичным применением для интеллектуального анализа текста является поиск в базах рефератов научных работ.
Например, в медицинском приложении можно найти все рефераты в базе данных касающиеся определенного синдрома.
Для этого составляют поисковую фразу или запрос с ключевыми словами, которые имеют отношение к синдрому.
Затем используется система поиска для сопоставления запроса с документами в базе данных и представляет пользователю документы,
которые являются релевантными и ранжируются в соответствии с релевантностью.

Очень широко распространённой областью применения интеллектуального анализа текста
являются поисковые системы в Интернете, где поисковая фраза обычно очень короткая,
и часто имеется так много соответствующих документов, что не представляется возможным предоставить их все пользователю.
В этом случае ранжирование результатов поиска имеет решающее значение для эффективности поисковой системы.

Методы кластеризации документов привлекают все больше и больше внимания
как основной и эффективный инструмент для организации, навигации, поиска и суммирования огромных объемов текстовых документов.
При хорошем методе кластеризации документов компьютеры могут автоматически организовывать наборы документов в осмысленную иерархию кластеров,
что обеспечивает эффективный просмотр и навигацию по документам.





\newpage





\section{Предварительная обработка документов}

При поиске информации ключевые слова,
содержащие информацию о содержании документа, называются термами.
Основным этапом поиска информации является создание списка всех термов в документе, так называемый индекс.
Для каждого терма хранится список всех предложений, которые содержат этот конкретный терм.
Это называется инвертированным индексом.

Но прежде чем создать индекс, необходимо выполнить два этапа предварительной обработки: исключить все стоп-слова и выделить основу оставшихся слов.



\subsection{Удаление стоп-слов}

Стоп-слова - это слова, которые можно найти практически в любом документе.
Следовательно, появление такого слова в документе не отличает этот документ от других документов.

\textit{Начало списка стоп-слов:}

a, будем, будет, будете, будешь, буду, будут, будучи, будь, будьте, бы,
был, была, были, было, быть, в, вам, вами, вас, весь, во, вот, все, всё,
всего, всей, всем, всём, всеми, всему, всех, всею, всея, всю, вся, вы,
да, для, до, его, ее, её, ей \ldots



\subsection{Выделение основы слова}

Выделение основы слова (стемминг) - это процесс сокращения каждого слова, которое сопряжено или имеет суффикс к своему основанию.
Ясно, что с точки зрения поиска информации никакая информация не теряется при таком сокращении.

\textit{Пример:}
\begin{align*}
  \textup{Адаптация} & \to \textup{Адапт} \\
  \textup{Адаптированный} & \\
  \textup{Адаптированный} & \\
  \textup{Адаптационный} & \\
  \textup{Адаптируйся}
\end{align*}

В ходе работы был реализован эффективный алгоритм построения индекса и удаления стоп-слов.
Также была использована библиотека nltk для выделения основы слова.

\newpage

Ниже приведена реализация алгоритма удаления стоп-слов и выделение основы слова на языке программирования \textit{Python 3}.
\\

\lstinputlisting
  [caption=Удаление стоп-слов и выделение основы слова, firstline=26, lastline=42]
  {../src/parser.py}

\newpage

Ниже приведена реализация алгоритма построения индекса на языке программирования \textit{Python 3}.
\\

\lstinputlisting
  [caption=Построение индекса, firstline=45, lastline=51]
  {../src/parser.py}





\newpage




\section{Построение модели векторного пространства}

Основная идея построения модели векторного пространства - создать терм-документную матрицу,
где каждое предложение представлено вектором-столбцом.
Столбец содержит ненулевые записи в позициях,
которые соответствуют термам, содержащимся в данном предложении.
Следовательно, каждая строка представляет терм и имеет ненулевые записи в тех позициях, которые соответствуют предложениям,
в которых этот терм можно найти.



\subsection{Построение взвешенной терм-документной матрицы}

Также распространено не только подсчитывать наличие термов в документах,
но ещё применять схему взвешивания термов, где элементы матрицы взвешиваются в зависимости от характеристик документа.
Например, можно определить элементы следующую схему взвешивания.
\begin{equation}
  a_{ij} = f_{ij} log(n /n_i),
\end{equation}
где $f_{ij}$ - частота термов или количество повторений терма $i$ в документе $j$, $n$ - общее количество документов,  а $n_i$ - количество документов, содержащих терм $i$.
Если терм встречается часто только в нескольких документах, то числитель и знаменатель велики.
В этом случае терм хорошо различим в разных группах документов, а логарифм придает ему больший вес в тех документах, где он появляется.

Обычно терм-документная матрица является разреженной с коэффициентом разреженности около $1\%$.
Также следует отметить что у матрицы могут быть полностью нулевые строки.
Это чаще всего проявляется при предварительной обработке диалогов или текстов,
содержащих простые предложения состоящие только из стоп-слов.

Учитывая всё вышесказанное, алгоритм построения взвешенной матрицы
был реализован с поддержкой разреженности результирующей матрицы
и для экономии памяти сразу оперирует с разреженными данными.

\newpage

Ниже приведена реализация алгоритма построения взвешенной матрицы на языке программирования \textit{Python 3}.
\\

\lstinputlisting
  [caption=Построение взвешенной терм-документной матрицы, firstline=72, lastline=147]
  {../src/parser.py}




\newpage




\section{Извлечение ключевых слов и предложений}

В связи с ростом объема доступной текстовой информации, необходимо иметь автоматические методы для резюмирования текста.
Самое распространённое применение - выдача поисковой системой соответствующих определенному запросу небольших кусков текста из каждого документа.

Автоматическое резюмирование текста - это активная область исследований, связанная с несколькими другими областями,
такими как поиск информации, обработка естественного языка и машинное обучение.
Неформальная цель резюмирования текста состоит в том, чтобы извлечь контент из текстового документа
и представить наиболее важный контент пользователю в сжатой форме и понятным пользователю способом.



\subsection{Оценка значимости}

Пусть $A$ - терм-документная матрица. Тогда, у нас есть матрица $A \in \RR^{m \times n}$,
где $m$ обозначает количество различных термов, а $n$ - количество предложений.
Элемент $a_{ij}$ определяется как количество повторений терма $i$ в предложении $j$.

Вектор-столбец $(a_{1j}, a_{2j} \ldots a_{mj})^T$ отличен от нуля в позициях, соответствующих термам в предложении $j$.
Точно так же вектор-строка $(a_{i1}, a_{i2} \ldots a_{in})$ ненулевой в позициях, соответствующих предложениям, содержащим терм $i$.

Терму $i$ присваивается неотрицательная оценка значимости, обозначаемая $u_i$.
Чем выше оценка значимости, тем важнее терм. Аналогично оценка значимости присваивается каждому предложению $j$ и обозначается как $v_j$.

Присвоение оценок значимости происходит согласно правилу взаимного подкрепления \cite{zha}.
Несложно доказать, что в случае разложения ранга $1$ результирующие векторы $w$ и $h$
(интерпретируем матрицу-столбец $W$ и матрицу-строку $H$ как векторы) задают оценки значимости для термов и предложений соответственно \cite{elden}.
Таким образом, для двух векторов $w$ и $h$ оценки значимости термов определяются как компоненты $w_i$,
а оценки значимости предложений являются компонентами $h_i$.



\newpage



\subsection{Извлечение ключевых предложений из аппроксимации ранга $k$}

Предположим, что мы нашли хорошее приближение ранга $k$ для терм-\\документной матрицы.
\begin{equation*}
  A \approx W H, \ W \in R^{m \times k}, \ H \in \RR^{k \times n}.
\end{equation*}

Значение ранга k должно быть выбрано не менее количества ключевых предложений, которые мы хотим извлечь.
$W$ является матрицей ранга $k$ базисных векторов,
и каждый столбец $H$ содержит координаты соответствующего столбца в $A$ в рамках базисных векторов.

Теперь воспользуемся тем фактом, что базисные векторы в $W$ представляют наиболее важные направления в «пространстве предложений» (пространстве столбцов).
Тем не менее, приближение низкого ранга не сразу указывают на то, какие предложения являются наиболее важными.
Эти предложения могут быть найдены, если мы сначала определим столбец $A$, который является \say{самым весомым} в рамках базиса,
то есть столбец в $H$ с наибольшей нормой. Это определяет один новый базисный вектор.
Затем мы выбираем столбец $H$, который является \say{самым весомым} с точки зрения оставшихся $k-1$ базисных векторов, и так далее.

Можно показать, что искомый алгоритм эквивалентен QR разложению с выбором главного элемента \cite{blas}.

\newpage

Ниже приведена реализация алгоритма выделения ключевых слов и предложений на языке программирования \textit{Python 3}.
\\

\lstinputlisting
  [caption=Выделение ключевых слов и предложений, firstline=0, lastline=999]
  {../src/extraction.py}
