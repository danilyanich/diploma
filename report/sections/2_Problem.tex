% НМФ (четко о постановке задачи, описание реализованных методов (отдельный пункт для каждого метода + код), о сходимости (см. статью))

\newpage
\section{Постановка задачи}

\subsection{Метод мультипликативного обновления}
\subsection{Метод попеременных наименьших квадратов}
\subsection{Метод попеременных наименьших квадратов}
\subsection{Метод попеременных неотрицательных наименьших квадратов}


Для матрицы $A \in R^{m \times n}, A \geq 0$ и числа $k \in N, k < min\{n, m\}$ необходимо найти матрицы $W \in R^{m \times k}, H \in R^{k \times n} : W \geq 0, H \geq 0$ чтобы свести к минимуму функционал:
$$
f(W, H) = \dfrac{||A - WH||_F}{|| A ||_F}
$$
Произведение $WH$ называется положительной факторизацией матрицы $A$. В идеальном случае, когда $f(W,H) = 0$ матрица $A$ равна произведению $WH$. На практике это далеко не всегда так, и почти всегда произведение $WH$ является приближенным разложением ранга не более $k$.


\newpage


\section{Исследования задачи}

Правильный выбор значения $k$ является критически важным для получения удовлетворительного результата, но также выбор $k$ очень часто зависит от природы решаемой задачи и поставленного вопроса. В большинстве случаев, $k$ обычно выбирают таким образом, чтобы $k \ll min\{m, n\}$, и в этом случае произведение $WH$ можно рассматривать как сжатую форму данных матрицы $A$.

Ключевой характеристикой положительной матричной факторизации является возможность использования численных методов для минимизации $f(W,H)​$ для извлечения некоторых базовых признаков в качестве базисных векторов матрицы $W​$, которые затем могут быть использованы для поиска и классификации. Ограничивая матрицы $W​$ и $H​$ только неотрицательными элементами положительная матричная факторизация позволяет представлять исходные данные из не вычитающих комбинаций. Элементы таких комбинаций могут быть частями лиц в изображениях, темами или кластерами в текстовых данных или некоторыми другими характеристиками многомерных данных.

Одной их важных проблем, влияющих на численную минимизацию $f(W, H)$, является существование локальных минимумов из-за невыпуклости $f(W, H)$ как по $W$, так и по $H$. Не менее важной проблемой является отсутствие единственного решения, которое можно легко увидеть рассматривая произведение $WDD^{− 1}H$ для любой неотрицательной обратимой матрицы $D$.

Несмотря на многие недостатки положительная матричная факторизация весьма привлекательна для приложений интеллектуального анализа данных, поскольку на практике даже локальные минимумы могут обеспечивать желаемые свойства, такие как сжатие данных и извлечение базовых признаков.


\section{Алгоритмы}

Численные алгоритмы положительной матричной факторизации могут быть поделены на 3 основные категории:
\begin{enumerate}
	\item Мультипликативные алгоритмы / Multiplicative update
	\item Алгоритмы градиентного спуска / Gradient descent
	\item Алгоритмы попеременных наименьших квадратов / Alternating least squares
\end{enumerate}

В ходе работы были исследованы 2 категории алгоритмов: мультипликативные алгоритмы и алгоритмы попеременных наименьших квадратов. Далее рассмотрим некоторые примеры из этих категорий подробнее.

Изложенные ниже алгоритмы предполагают изначальное заполнение матриц $W$ и $H$ случайными, неотрицательными числами.


\newpage


\subsection{Multiplicative update rule}

Алгоритм был разработан и улучшен Daniel D. Lee и H. Sebastian Seung в 2001 году.

\begin{align*}
	& H_{i+1} = H_i * W_i^T A / (W_i^T W_i H_i + \epsilon) \\
	& W_{i+1} = W_i * A H_{i+1}^T / (W_i H_{i+1} H_{i+1}^T + \epsilon) & i = 1, 2, ...
\end{align*}

Ниже приведена реализация алгоритма на `python3`:

\verbatimfont{\small}
\begin{verbatim}
eps = 10e-9

while True:
  # H = H * (W' A) / ((W' W H) + 10e-9)
  expr1 = W.T @ A
  expr2 = (W.T @ W) @ H + eps
  H = element_wise_divide(element_wise_multiply(H, expr1), expr2)

  # W = W * (A H') / ((W H H') + 10e-9)
  expr3 = A @ H.T
  expr4 = W @ (H @ H.T) + eps
  W = element_wise_divide(element_wise_multiply(W, expr3), expr4)

  yield relative_error(A, W, H)
\end{verbatim}


\newpage


\subsection{Alternating least squares}

В этом алгоритме за шагом наименьших квадратов следует другой шаг наименьших квадратов поочередно. Алгоритм использует тот факт, что задача минимизации выражения $f(W, H)$ не является выпуклой как в $W$, так и в $H$ одновременно, но является выпуклой в $W$ или $H$ по отдельности. Таким образом, для одной матрицы другая матрица может быть найдена с простым вычисление наименьших квадратов.

\begin{align*}
	& H_{i+1} = solve(W_i^T W_i H_{i+1} = W_i^T A) \\
	& H_{i+1} = nonnegative(H_{i+1}) \\
	& W_{i+1}^T = solve(H_{i+1} H_{i+1}^T W_{i+1}^T = H_{i+1} A^T) \\
	& W_{i+1} = nonnegative(W_{i+1}) & i = 1, 2, ...
\end{align*}

Ниже приведена реализация алгоритма на `python3`:

\verbatimfont{\small}
\begin{verbatim}
while True:
  # W' W H = W' A
  H = solve(W.T @ W, W.T @ A)
  H = nonnegative(H)

  # H H' W' = H A'
  WT = solve(H @ H.T, H @ A.T)
  W = nonnegative(WT.T)

  yield relative_error(A, W, H)
\end{verbatim}


\newpage


\subsection{Алгоритм инициализации матриц $W$ и $H$}

Проблема изложенных алгоритмов положительной матричной факторизации заключается в том, что сходимость к глобальному минимуму не гарантируется. Часто случается, что сходимость медленная и достигается неоптимальное приближение.

Эффективная процедура для вычисления хорошего начального приближения может быть основана на сингулярном разложении матрицы $A$.

Если $A = U \Sigma V^T $ является сингулярным разложением для матрицы $A$, мы можем взять первый сингулярный вектор $u_1$ в качестве первого столбца в $W$. Затем мы вычисляем матрицу $C = u_2 v_2^T$ и заменяем все отрицательные её элементы на ноль. Т.к. $C$ теперь является неотрицательной матрицей, то мы знаем, что первый сингулярный вектор этой матрицы также неотрицательный. Кроме того, мы можем надеяться, что это достаточно хорошее приближение $u_2$, поэтому мы можем принять его за второй столбец $W$. Продолжим процедуру дальше, пока не заполним всю матрицу $W$.

Ниже приведена реализация алгоритма на `python3`:

\verbatimfont{\small}
\begin{verbatim}
rows, columns = A.shape

U, _, VT = svd_factorization(A, rank_k)

W = U[:,:rank_k]
H = random_matrix(rank_k, columns)

for j in range(1, rank_k):
  C = reshape(U[:,j], (rows, 1)) @ reshape(VT[j,:], (1, columns))
  C = nonnegative(C)
  u, _, _ = svd_factorization(C, 1)

  W[:,j] = u[:,0]
return W, H
\end{verbatim}
